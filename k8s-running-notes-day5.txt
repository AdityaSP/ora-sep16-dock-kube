Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ kubectl version
Client Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.0", GitCommit:"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529", GitTreeState:"clean", BuildDate:"2019-06-19T16:40:16Z", GoVersion:"go1.12.5", Compiler:"gc", Platform:"windows/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.0", GitCommit:"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77", GitTreeState:"clean", BuildDate:"2019-09-18T14:27:17Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"linux/amd64"}
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ kubectl get nodes
NAME        STATUS     ROLES    AGE   VERSION
k8smaster   NotReady   master   17h   v1.16.0
k8snode1    NotReady   <none>   17h   v1.16.0
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ minikube start
* minikube v1.2.0 on windows (amd64)
* Tip: Use 'minikube start -p <name>' to create a new cluster, or 'minikube delete' to delete this one.
* Restarting existing virtualbox VM for "minikube" ...
* Waiting for SSH access ...
* Configuring environment for Kubernetes v1.15.0 on Docker 18.09.6
* Relaunching Kubernetes v1.15.0 using kubeadm ...
* Verifying: apiserver proxy etcd scheduler controller dns
* Done! kubectl is now configured to use "minikube"
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
*         minikube                      minikube     minikube
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    <none>   46d   v1.15.0
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ kubectl config use-context kubernetes-admin@kubernetes
Switched to context "kubernetes-admin@kubernetes".
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          minikube                      minikube     minikube
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ kubectl get nodes
NAME        STATUS     ROLES    AGE   VERSION
k8smaster   NotReady   master   17h   v1.16.0
k8snode1    NotReady   <none>   17h   v1.16.0
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ



Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ kubectl config use-context kubernetes-admin@kubernetes
Switched to context "kubernetes-admin@kubernetes".
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ kubectl get nodes
NAME        STATUS     ROLES    AGE   VERSION
k8smaster   NotReady   master   17h   v1.16.0
k8snode1    NotReady   <none>   17h   v1.16.0
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ cd newcluster/
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ ls
canal.yml  kube-installations.sh*  kube-master.sh  master-output.txt  node-output.txt  rbac.yml  readme.md  setup.txt  Vagrantfile
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ ls canal.yml rbac.yml
canal.yml  rbac.yml
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ vi canal.yml
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ ls canal.yml rbac.yml
canal.yml  rbac.yml
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl apply -f rbac.yml
clusterrole.rbac.authorization.k8s.io/calico created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/canal-flannel created
clusterrolebinding.rbac.authorization.k8s.io/canal-calico created
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl apply -f canal.yml
configmap/canal-config created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
serviceaccount/canal created
error: unable to recognize "canal.yml": no matches for kind "DaemonSet" in version "extensions/v1beta1"
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl apiversions
Error: unknown command "apiversions" for "kubectl"

Did you mean this?
        api-versions

Run 'kubectl --help' for usage.
unknown command "apiversions" for "kubectl"

Did you mean this?
        api-versions

Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl api-versions
admissionregistration.k8s.io/v1
admissionregistration.k8s.io/v1beta1
apiextensions.k8s.io/v1
apiextensions.k8s.io/v1beta1
apiregistration.k8s.io/v1
apiregistration.k8s.io/v1beta1
apps/v1
authentication.k8s.io/v1
authentication.k8s.io/v1beta1
authorization.k8s.io/v1
authorization.k8s.io/v1beta1
autoscaling/v1
autoscaling/v2beta1
autoscaling/v2beta2
batch/v1
batch/v1beta1
certificates.k8s.io/v1beta1
coordination.k8s.io/v1
coordination.k8s.io/v1beta1
crd.projectcalico.org/v1
events.k8s.io/v1beta1
extensions/v1beta1
networking.k8s.io/v1
networking.k8s.io/v1beta1
node.k8s.io/v1beta1
policy/v1beta1
rbac.authorization.k8s.io/v1
rbac.authorization.k8s.io/v1beta1
scheduling.k8s.io/v1
scheduling.k8s.io/v1beta1
storage.k8s.io/v1
storage.k8s.io/v1beta1
v1
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl version
Client Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.0", GitCommit:"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529", GitTreeState:"clean", BuildDate:"2019-06-19T16:40:16Z", GoVersion:"go1.12.5", Compiler:"gc", Platform:"windows/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.0", GitCommit:"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77", GitTreeState:"clean", BuildDate:"2019-09-18T14:27:17Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"linux/amd64"}
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl delete -f canal.yml
configmap "canal-config" deleted
customresourcedefinition.apiextensions.k8s.io "felixconfigurations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "bgpconfigurations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "clusterinformations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "networkpolicies.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "globalnetworksets.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "hostendpoints.crd.projectcalico.org" deleted
Error from server (NotFound): error when deleting "canal.yml": customresourcedefinitions.apiextensions.k8s.io "ippools.crd.projectcalico.org" not
found
Error from server (NotFound): error when deleting "canal.yml": customresourcedefinitions.apiextensions.k8s.io "globalnetworkpolicies.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "canal.yml": serviceaccounts "canal" not found
unable to recognize "canal.yml": no matches for kind "DaemonSet" in version "extensions/v1beta1"
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl delete -f rbac.yml
clusterrole.rbac.authorization.k8s.io "calico" deleted
clusterrole.rbac.authorization.k8s.io "flannel" deleted
clusterrolebinding.rbac.authorization.k8s.io "canal-flannel" deleted
clusterrolebinding.rbac.authorization.k8s.io "canal-calico" deleted
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get ds -n kube-system
NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
kube-proxy   2         2         2       2            2           beta.kubernetes.io/os=linux   18h
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get ds -n kube-system -o yaml
apiVersion: v1
items:
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2019-09-19T10:48:11Z"
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "548"
    selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy
    uid: 456d5b34-50c7-4370-b030-4064505cdf38
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --config=/var/lib/kube-proxy/config.conf
          - --hostname-override=$(NODE_NAME)
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: k8s.gcr.io/kube-proxy:v1.16.0
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          beta.kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ vi canal.yml
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ grep DaemonSet canal.yml
kind: DaemonSet
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ vi canal.yml
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl apply -f rbac.yml
clusterrole.rbac.authorization.k8s.io/calico created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/canal-flannel created
clusterrolebinding.rbac.authorization.k8s.io/canal-calico created
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl apply -f canal.yml
configmap/canal-config created
daemonset.apps/canal created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
serviceaccount/canal created
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods
No resources found.
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl ns
Error: unknown command "ns" for "kubectl"

Did you mean this?
        cp

Run 'kubectl --help' for usage.
unknown command "ns" for "kubectl"

Did you mean this?
        cp

Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get ns
NAME              STATUS   AGE
default           Active   18h
kube-node-lease   Active   18h
kube-public       Active   18h
kube-system       Active   18h
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods
No resources found.
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods -ns kube-system
Error from server (NotFound): namespaces "s" not found
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods -n kube-system
NAME                                READY   STATUS    RESTARTS   AGE
canal-9ww9w                         3/3     Running   0          7m53s
canal-wg87b                         3/3     Running   0          7m53s
coredns-5644d7b6d9-jssjx            1/1     Running   0          18h
coredns-5644d7b6d9-mh599            1/1     Running   0          18h
etcd-k8smaster                      1/1     Running   1          18h
kube-apiserver-k8smaster            1/1     Running   1          18h
kube-controller-manager-k8smaster   1/1     Running   1          18h
kube-proxy-2wzsp                    1/1     Running   1          18h
kube-proxy-s4t5c                    1/1     Running   1          18h
kube-scheduler-k8smaster            1/1     Running   1          18h
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get daemonset
No resources found.
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get daemonset -n kube-system
NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
canal        2         2         2       2            2           <none>                        8m50s
kube-proxy   2         2         2       2            2           beta.kubernetes.io/os=linux   18h
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get daemonset -n kube-system kube-proxy -o yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  annotations:
    deprecated.daemonset.template.generation: "1"
  creationTimestamp: "2019-09-19T10:48:11Z"
  generation: 1
  labels:
    k8s-app: kube-proxy
  name: kube-proxy
  namespace: kube-system
  resourceVersion: "548"
  selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy
  uid: 456d5b34-50c7-4370-b030-4064505cdf38
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kube-proxy
  template:
    metadata:
      creationTimestamp: null
      labels:
        k8s-app: kube-proxy
    spec:
      containers:
      - command:
        - /usr/local/bin/kube-proxy
        - --config=/var/lib/kube-proxy/config.conf
        - --hostname-override=$(NODE_NAME)
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        image: k8s.gcr.io/kube-proxy:v1.16.0
        imagePullPolicy: IfNotPresent
        name: kube-proxy
        resources: {}
        securityContext:
          privileged: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/kube-proxy
          name: kube-proxy
        - mountPath: /run/xtables.lock
          name: xtables-lock
        - mountPath: /lib/modules
          name: lib-modules
          readOnly: true
      dnsPolicy: ClusterFirst
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/os: linux
      priorityClassName: system-node-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: kube-proxy
      serviceAccountName: kube-proxy
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - operator: Exists
      volumes:
      - configMap:
          defaultMode: 420
          name: kube-proxy
        name: kube-proxy
      - hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
        name: xtables-lock
      - hostPath:
          path: /lib/modules
          type: ""
        name: lib-modules
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
status:
  currentNumberScheduled: 2
  desiredNumberScheduled: 2
  numberAvailable: 2
  numberMisscheduled: 0
  numberReady: 2
  observedGeneration: 1
  updatedNumberScheduled: 2
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ grep DaemonSet canal.yml
kind: DaemonSet
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ vi canal.yml
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ vi canal.yml
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get daemonset -n kube-system
NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
canal        2         2         2       2            2           <none>                        13m
kube-proxy   2         2         2       2            2           beta.kubernetes.io/os=linux   18h
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get daemonset -n kube-system -o yaml kube-proxy
apiVersion: apps/v1
kind: DaemonSet
metadata:
  annotations:
    deprecated.daemonset.template.generation: "1"
  creationTimestamp: "2019-09-19T10:48:11Z"
  generation: 1
  labels:
    k8s-app: kube-proxy
  name: kube-proxy
  namespace: kube-system
  resourceVersion: "548"
  selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy
  uid: 456d5b34-50c7-4370-b030-4064505cdf38
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kube-proxy
  template:
    metadata:
      creationTimestamp: null
      labels:
        k8s-app: kube-proxy
    spec:
      containers:
      - command:
        - /usr/local/bin/kube-proxy
        - --config=/var/lib/kube-proxy/config.conf
        - --hostname-override=$(NODE_NAME)
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        image: k8s.gcr.io/kube-proxy:v1.16.0
        imagePullPolicy: IfNotPresent
        name: kube-proxy
        resources: {}
        securityContext:
          privileged: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/kube-proxy
          name: kube-proxy
        - mountPath: /run/xtables.lock
          name: xtables-lock
        - mountPath: /lib/modules
          name: lib-modules
          readOnly: true
      dnsPolicy: ClusterFirst
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/os: linux
      priorityClassName: system-node-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: kube-proxy
      serviceAccountName: kube-proxy
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - operator: Exists
      volumes:
      - configMap:
          defaultMode: 420
          name: kube-proxy
        name: kube-proxy
      - hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
        name: xtables-lock
      - hostPath:
          path: /lib/modules
          type: ""
        name: lib-modules
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
status:
  currentNumberScheduled: 2
  desiredNumberScheduled: 2
  numberAvailable: 2
  numberMisscheduled: 0
  numberReady: 2
  observedGeneration: 1
  updatedNumberScheduled: 2
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          minikube                      minikube     minikube
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods
NAME                       READY   STATUS              RESTARTS   AGE
app-dep-75cb47b759-fdgtb   1/1     Running             0          14s
app-dep-75cb47b759-jslct   0/1     ContainerCreating   0          14s
app-dep-75cb47b759-zzrqs   1/1     Running             0          15s
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get svc
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
app-dep-svc   NodePort    10.111.44.29   <none>        9999:32007/TCP   24s
kubernetes    ClusterIP   10.96.0.1      <none>        443/TCP          20h
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
app-dep-ccc544dbb-64wh9   1/1     Running   0          20s
app-dep-ccc544dbb-jz7tb   1/1     Running   0          20s
app-dep-ccc544dbb-m7b42   1/1     Running   0          5m20s
app-dep-ccc544dbb-s7r8l   1/1     Running   0          5m15s
app-dep-ccc544dbb-z4sg5   1/1     Running   0          5m25s
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl rollout history deploy app-dep
deployment.apps/app-dep
REVISION  CHANGE-CAUSE
1         <none>
2         <none>

Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl rollout history deploy app-dep --to-revision 1
Error: unknown flag: --to-revision


Examples:
  # View the rollout history of a deployment
  kubectl rollout history deployment/abc

  # View the details of daemonset revision 3
  kubectl rollout history daemonset/abc --revision=3

Options:
      --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats.
  -f, --filename=[]: Filename, directory, or URL to files identifying the resource to get from a server.
  -k, --kustomize='': Process the kustomization directory. This flag can't be used together with -f or -R.
  -o, --output='': Output format. One of: json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-file.
  -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests organized within the same directory.
      --revision=0: See the details, including podTemplate of the revision specified
      --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].

Usage:
  kubectl rollout history (TYPE NAME | TYPE/NAME) [flags] [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).

unknown flag: --to-revision
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl rollout history deploy app-dep --revision 1
deployment.apps/app-dep with revision #1
Pod Template:
  Labels:       app=cicd
        pod-template-hash=75cb47b759
  Containers:
   web:
    Image:      adityaprabhakara/sept20app:4
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl rollout history deploy app-dep --revision 2
deployment.apps/app-dep with revision #2
Pod Template:
  Labels:       app=cicd
        pod-template-hash=ccc544dbb
  Containers:
   web:
    Image:      adityaprabhakara/sept20app:5
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
app-dep-ccc544dbb-64wh9   1/1     Running   0          75m   192.168.1.75   k8snode1   <none>           <none>
app-dep-ccc544dbb-jz7tb   1/1     Running   0          75m   192.168.1.76   k8snode1   <none>           <none>
app-dep-ccc544dbb-m7b42   1/1     Running   0          80m   192.168.1.73   k8snode1   <none>           <none>
app-dep-ccc544dbb-s7r8l   1/1     Running   0          80m   192.168.1.74   k8snode1   <none>           <none>
app-dep-ccc544dbb-z4sg5   1/1     Running   0          80m   192.168.1.72   k8snode1   <none>           <none>
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl taint node k8smaster node-role.kubernetes.io/master:NoSchedule-
error: taint "node-role.kubernetes.io/master:NoSchedule" not found
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
app-dep-ccc544dbb-64wh9   1/1     Running   0          77m   192.168.1.75   k8snode1   <none>           <none>
app-dep-ccc544dbb-jz7tb   1/1     Running   0          77m   192.168.1.76   k8snode1   <none>           <none>
app-dep-ccc544dbb-m7b42   1/1     Running   0          82m   192.168.1.73   k8snode1   <none>           <none>
app-dep-ccc544dbb-s7r8l   1/1     Running   0          82m   192.168.1.74   k8snode1   <none>           <none>
app-dep-ccc544dbb-z4sg5   1/1     Running   0          82m   192.168.1.72   k8snode1   <none>           <none>
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl scale deploy app-dep --replicas 1
deployment.apps/app-dep scaled
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods -o wide
NAME                      READY   STATUS        RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
app-dep-ccc544dbb-64wh9   0/1     Terminating   0          77m   <none>         k8snode1   <none>           <none>
app-dep-ccc544dbb-jz7tb   0/1     Terminating   0          77m   192.168.1.76   k8snode1   <none>           <none>
app-dep-ccc544dbb-m7b42   0/1     Terminating   0          82m   192.168.1.73   k8snode1   <none>           <none>
app-dep-ccc544dbb-s7r8l   0/1     Terminating   0          82m   192.168.1.74   k8snode1   <none>           <none>
app-dep-ccc544dbb-z4sg5   1/1     Running       0          82m   192.168.1.72   k8snode1   <none>           <none>
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods -o wide
NAME                      READY   STATUS        RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
app-dep-ccc544dbb-64wh9   0/1     Terminating   0          77m   <none>         k8snode1   <none>           <none>
app-dep-ccc544dbb-jz7tb   0/1     Terminating   0          77m   192.168.1.76   k8snode1   <none>           <none>
app-dep-ccc544dbb-m7b42   0/1     Terminating   0          82m   192.168.1.73   k8snode1   <none>           <none>
app-dep-ccc544dbb-s7r8l   0/1     Terminating   0          82m   192.168.1.74   k8snode1   <none>           <none>
app-dep-ccc544dbb-z4sg5   1/1     Running       0          83m   192.168.1.72   k8snode1   <none>           <none>
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
app-dep-ccc544dbb-z4sg5   1/1     Running   0          83m   192.168.1.72   k8snode1   <none>           <none>
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl scale deploy app-dep --replicas 5
deployment.apps/app-dep scaled
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods -o wide
NAME                      READY   STATUS              RESTARTS   AGE   IP             NODE        NOMINATED NODE   READINESS GATES
app-dep-ccc544dbb-9c44r   0/1     ContainerCreating   0          2s    <none>         k8smaster   <none>           <none>
app-dep-ccc544dbb-mp6sm   0/1     ContainerCreating   0          2s    <none>         k8snode1    <none>           <none>
app-dep-ccc544dbb-nv4qs   0/1     ContainerCreating   0          2s    <none>         k8snode1    <none>           <none>
app-dep-ccc544dbb-vl4l6   0/1     ContainerCreating   0          2s    <none>         k8snode1    <none>           <none>
app-dep-ccc544dbb-z4sg5   1/1     Running             0          83m   192.168.1.72   k8snode1    <none>           <none>
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE   IP             NODE        NOMINATED NODE   READINESS GATES
app-dep-ccc544dbb-9c44r   1/1     Running   0          15s   192.168.0.25   k8smaster   <none>           <none>
app-dep-ccc544dbb-mp6sm   1/1     Running   0          15s   192.168.1.79   k8snode1    <none>           <none>
app-dep-ccc544dbb-nv4qs   1/1     Running   0          15s   192.168.1.78   k8snode1    <none>           <none>
app-dep-ccc544dbb-vl4l6   1/1     Running   0          15s   192.168.1.77   k8snode1    <none>           <none>
app-dep-ccc544dbb-z4sg5   1/1     Running   0          83m   192.168.1.72   k8snode1    <none>           <none>
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl exec -it app-dep-ccc544dbb-9c44r -- sh
/ # ping 192.168.1.78
PING 192.168.1.78 (192.168.1.78): 56 data bytes
64 bytes from 192.168.1.78: seq=0 ttl=62 time=1.261 ms
64 bytes from 192.168.1.78: seq=1 ttl=62 time=0.928 ms
64 bytes from 192.168.1.78: seq=2 ttl=62 time=0.667 ms
^C
--- 192.168.1.78 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 0.667/0.952/1.261 ms
/ # exit
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl taint node k8smaster node-role.kubernetes.io/master:NoSchedule
error: at least one taint update is required
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl taint node k8smaster node-role.kubernetes.io/master:NoSchedule=true
error: invalid taint effect: NoSchedule=true, unsupported taint effect
See 'kubectl taint -h' for help and examples
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl taint node k8smaster node-role.kubernetes.io/master:NoSchedule
error: at least one taint update is required
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl taint --help
Update the taints on one or more nodes.

  *  A taint consists of a key, value, and effect. As an argument here, it is expressed as key=value:effect.
  *  The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to
253 characters.
  *  Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app
  *  The value is optional. If given, it must begin with a letter or number, and may contain letters, numbers, hyphens,
dots, and underscores, up to  63 characters.
  *  The effect must be NoSchedule, PreferNoSchedule or NoExecute.
  *  Currently taint can only apply to node.

Examples:
  # Update node 'foo' with a taint with key 'dedicated' and value 'special-user' and effect 'NoSchedule'.
  # If a taint with that key and effect already exists, its value is replaced as specified.
  kubectl taint nodes foo dedicated=special-user:NoSchedule

  # Remove from node 'foo' the taint with key 'dedicated' and effect 'NoSchedule' if one exists.
  kubectl taint nodes foo dedicated:NoSchedule-

  # Remove from node 'foo' all the taints with key 'dedicated'
  kubectl taint nodes foo dedicated-

  # Add a taint with key 'dedicated' on nodes having label mylabel=X
  kubectl taint node -l myLabel=X  dedicated=foo:PreferNoSchedule

  # Add to node 'foo' a taint with key 'bar' and no value
  kubectl taint nodes foo bar:NoSchedule

Options:
      --all=false: Select all nodes in the cluster
      --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in
the template. Only applies to golang and jsonpath output formats.
  -o, --output='': Output format. One of:
json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-file.
      --overwrite=false: If true, allow taints to be overwritten, otherwise reject taint updates that overwrite existing
taints.
  -l, --selector='': Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2)
      --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The
template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].
      --validate=true: If true, use a schema to validate the input before sending it

Usage:
  kubectl taint NODE NAME KEY_1=VAL_1:TAINT_EFFECT_1 ... KEY_N=VAL_N:TAINT_EFFECT_N [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl taint node k8smaster node-role.kubernetes.io/master:NoSchedule
error: at least one taint update is required
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl taint node k8smaster node-role.kubernetes.io/master:NoSchedule=1
error: invalid taint effect: NoSchedule=1, unsupported taint effect
See 'kubectl taint -h' for help and examples
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ kubectl taint node k8smaster node-role.kubernetes.io/master:NoSchedule
error: at least one taint update is required
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/newcluster (master)
λ cd ..
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ ls
blog/  build-machine/  dep/  newcluster/  pod/  pv-pvc/  rc/  readme.txt  resource-quota/  service/
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s (master)
λ cd pv-pvc/
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/pv-pvc (master)
λ ls
mysql.yml  pv.yml  pvc.yml
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/pv-pvc (master)
λ kubectl apply -f pv.yml
persistentvolume/mysql-pv-volume2 created
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/pv-pvc (master)
λ kubectl get pv
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
mysql-pv-volume2   2Gi        RWO            Retain           Available           manual                  6s
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/pv-pvc (master)
λ kubectl get pvc
No resources found.
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/pv-pvc (master)
λ kubectl apply -f pvc.yml
persistentvolumeclaim/mysql-pv-claim1 created
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/pv-pvc (master)
λ kubectl get pv
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE
mysql-pv-volume2   2Gi        RWO            Retain           Bound    default/mysql-pv-claim1   manual                  2m29s
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/pv-pvc (master)
λ kubectl get pvc
NAME              STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mysql-pv-claim1   Bound    mysql-pv-volume2   2Gi        RWO            manual         34s
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/pv-pvc (master)
λ kubectl apply -f mysql.yml
deployment.apps/mysql created
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/pv-pvc (master)
λ kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
app-dep-ccc544dbb-9c44r   1/1     Running   0          31m
app-dep-ccc544dbb-mp6sm   1/1     Running   0          31m
app-dep-ccc544dbb-nv4qs   1/1     Running   0          31m
app-dep-ccc544dbb-vl4l6   1/1     Running   0          31m
app-dep-ccc544dbb-z4sg5   1/1     Running   0          114m
mysql-664bc554d7-dblmn    1/1     Running   0          5s
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/pv-pvc (master)
λ kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE    IP             NODE        NOMINATED NODE   READINESS GATES
app-dep-ccc544dbb-9c44r   1/1     Running   0          31m    192.168.0.25   k8smaster   <none>           <none>
app-dep-ccc544dbb-mp6sm   1/1     Running   0          31m    192.168.1.79   k8snode1    <none>           <none>
app-dep-ccc544dbb-nv4qs   1/1     Running   0          31m    192.168.1.78   k8snode1    <none>           <none>
app-dep-ccc544dbb-vl4l6   1/1     Running   0          31m    192.168.1.77   k8snode1    <none>           <none>
app-dep-ccc544dbb-z4sg5   1/1     Running   0          115m   192.168.1.72   k8snode1    <none>           <none>
mysql-664bc554d7-dblmn    1/1     Running   0          40s    192.168.1.80   k8snode1    <none>           <none>
Aditya@DESKTOP-F88S8GU ~/docker-workshop/sept16/ora-sep16-dock-kube/k8s/pv-pvc (master)
λ
